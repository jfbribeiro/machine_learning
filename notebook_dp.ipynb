{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# TOC<a class=\"anchor\"><a id='toc'></a></b><br>\n",
    "* [<font color='#E8800A'>Naive Bayes</font>](#first-bullet) <br>\n",
    "- [<font color='#E8800A'>Logistic Regression</font>](#second-bullet)<br>\n",
    "- [<font color='#E8800A'>KNN</font>](#third-bullet)<br>\n",
    "- [<font color='#E8800A'>Support Vector Machines</font>](#fourth-bullet)<br>\n",
    "- [<font color='#E8800A'>Decision Trees</font>](#fifth-bullet)<br>\n",
    "- [<font color='#E8800A'>Random forest</font>](#sixth-bullet)<br>\n",
    "- [<font color='#E8800A'>Boosted Trees</font>](#seventh-bullet)<br> \n",
    "- [<font color='#E8800A'>Neural Networks</font>](#eighth-bullet)<br> \n",
    "- [<font color='#E8800A'>Ensembles</font>](#ninth-bullet)<br>   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score,\\\n",
    "RandomizedSearchCV, KFold, StratifiedKFold\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, \\\n",
    "GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "\n",
    "%pip install boruta\n",
    "from boruta import BorutaPy\n",
    "\n",
    "%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "%pip install pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "%pip install openpyxl\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "datain_path = 'data/'\n",
    "src_path = 'src/'\n",
    "\n",
    "\n",
    "explorations_path = 'explorations/'\n",
    "submissions_path = 'submissions/'\n",
    "comparison_path = 'comparison/'\n",
    "\n",
    "paths = [explorations_path, submissions_path, comparison_path]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': 'Train.xlsx',\n",
    "    'test':'Test.xlsx', \n",
    "    'both': {\n",
    "        'train': 'Train.xlsx',   \n",
    "        'test':'Test.xlsx',\n",
    "    } \n",
    "}\n",
    "\n",
    "#datasets = pd.DataFrame(datasets, columns=['name', 'path']).set_index('name')\n",
    "\n",
    "dataset_name = 'both'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'both': \n",
    "    data = pd.DataFrame()\n",
    "    for dataset_path in datasets[dataset_name].values(): \n",
    "        tmp = pd.read_excel(os.path.join(datain_path, dataset_path))\n",
    "        data = pd.concat([data, tmp])\n",
    "\n",
    "else:    \n",
    "    dataset_path = datasets[dataset_name]\n",
    "    data = pd.read_excel(os.path.join(datain_path, dataset_path))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profile report\n",
    "\n",
    "profile = ProfileReport(\n",
    "    data,\n",
    "    title='Raw data',\n",
    "    minimal=False, \n",
    "    correlations={\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": False},\n",
    "    \"kendall\": {\"calculate\": False},\n",
    "    \"phi_k\": {\"calculate\": False},\n",
    "    \"cramers\": {\"calculate\": False},\n",
    "    }\n",
    ")\n",
    "profile.to_file(os.path.join(explorations_path, 'profile_data_raw.html'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature=10\n",
    "\n",
    "def calc_elbowdata(data, base_col): \n",
    "    plotdata = data.copy().loc[~data.Income.isna(),:].groupby(base_col).size().sort_values(ascending=False)\n",
    "    plotdata = pd.DataFrame(plotdata / plotdata.sum()).cumsum().rename(columns={0:'nobs_rel'})\n",
    "    return plotdata\n",
    "\n",
    "def get_feature_imp_by_expl(data, base_col, n_feature=n_feature): \n",
    "    \n",
    "    # make sure every combination of levels exist, fill with 0 if no obs\n",
    "    base = data[base_col].unique()\n",
    "    Income = [0, 1]\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [base, Income],\n",
    "        names=[base_col, 'Income']\n",
    "    )\n",
    "\n",
    "    pd1 = pd.DataFrame(index=idx)\n",
    "    \n",
    "\n",
    "    a = data.groupby([base_col, 'Income']).size().to_frame().rename(columns={0:'nobs'})\n",
    "    \n",
    "    a = pd.concat([pd1, a], axis=1)\n",
    "    a.loc[a.nobs.isna(), 'nobs'] = 0\n",
    "    \n",
    "    a['nobs_rel'] = a.groupby(level=base_col).transform(lambda x: x / (x[0] + x[1]))\n",
    "    value_cols = a.columns.to_list()\n",
    "    a.reset_index(inplace=True)\n",
    "    \n",
    "    # top Features by nobs: \n",
    "    topFeat = data.groupby(base_col).size().to_frame().rename(columns={0:'nobs'})\\\n",
    "        .sort_values('nobs', ascending=False).iloc[0:n_feature,:]\\\n",
    "        .index.to_list()\n",
    "    \n",
    "    a.sort_values(['nobs', base_col], ascending=False, inplace=True)\n",
    "    \n",
    "    #print('len(topFeat)', len(topFeat))\n",
    "    #print('len(a)', len(a))\n",
    "    #print('n_feature', n_feature)\n",
    "    #print('a[base_col].nunique()', a[base_col].nunique())\n",
    "\n",
    "    if len(topFeat) < a[base_col].nunique(): \n",
    "        print(f'***Features Filtered to top_{n_feature} by nobs!***')\n",
    "    #print(a[base_col].nunique())\n",
    "\n",
    "    a = a.loc[a[base_col].isin(topFeat),:]\n",
    "    return a, value_cols\n",
    "\n",
    "def plot_feature_imp_by_expl(data, base_col, saveplots): \n",
    "    \n",
    "    a, value_cols = get_feature_imp_by_expl(data, base_col)\n",
    "    \n",
    "    value_cols.remove('nobs')\n",
    "    print(value_cols)\n",
    "    n_plots = len(value_cols) + 1\n",
    "\n",
    "    fig, ax = plt.subplots() #, figsize=(20,7)\n",
    "    #sns.barplot(data=a, x=base_col, y='nobs', hue='Income', ax=ax)#.set_title(col) # [0:10]\n",
    "    #ax.tick_params(labelrotation=45)\n",
    "    plotdata = calc_elbowdata(data, base_col)\n",
    "    sns.lineplot(data=plotdata, y=plotdata.index, x='nobs_rel', ax=ax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if saveplots: \n",
    "        plt.savefig(os.path.join(explorations_path, 'feature_imp_by_expl_abs.png'), dpi=200)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols = n_plots, gridspec_kw={'width_ratios': [3,1]}) #, figsize=(20,7)\n",
    "    for i, col in enumerate(value_cols): \n",
    "        sns.barplot(data=a, x=base_col, y=col, hue='Income', ax=ax[i])#.set_title(col) # [0:10]\n",
    "        ax[i].tick_params(labelrotation=45)\n",
    "\n",
    "    sns.countplot(data=data, x='Income', ax=ax[n_plots-1])\n",
    "    plt.tight_layout()\n",
    "    if saveplots: \n",
    "        plt.savefig(os.path.join(explorations_path, 'feature_imp_by_expl_rel.png'), dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "def get_target_ratio(data):  \n",
    "    a = data.groupby(['Income']).size().to_frame().rename(columns={0:'nobs'})\n",
    "    a['nobs_rel'] = a.transform(lambda x: x / (x[0] + x[1]))\n",
    "    value_cols = a.columns.to_list()\n",
    "    a.reset_index(inplace=True)\n",
    "    return a.loc[a.Income == 1, 'nobs_rel'].to_list()[0]\n",
    "\n",
    "\n",
    "def get_feature_imp_by_target_ratio(data, base_col, weighted=False, saveplots=False): \n",
    "\n",
    "    target_ratio = get_target_ratio(data)\n",
    "    target_ratio\n",
    "                                            \n",
    "    a, _ = get_feature_imp_by_expl(data, base_col, n_feature=100)\n",
    "    \n",
    "    #########\n",
    "    nObsPerFeatClass =  data.groupby([base_col]).size().to_frame().rename(columns={0:'nobs'})\n",
    "                           \n",
    "\n",
    "    ratio_per_level = a.loc[a.Income == 1, [base_col,'nobs_rel']]\\\n",
    "        .set_index(base_col)\\\n",
    "        .rename(columns={'nobs_rel':'class1_ratio'})\n",
    "\n",
    "    ratio_per_level = pd.concat([ratio_per_level, nObsPerFeatClass], axis=1)\n",
    "    #min_max_scaler_obs = MinMaxScaler()\n",
    "    ratio_per_level['nobs_rel'] = ratio_per_level.nobs / sum(ratio_per_level.nobs)\n",
    "\n",
    "\n",
    "    \n",
    "    ratio_per_level['diff_to_target'] = ratio_per_level['class1_ratio'] - target_ratio\n",
    "    ratio_per_level['diff_to_target_dir'] = ['neg' if obs < 0 else 'pos' for obs in ratio_per_level['diff_to_target']]\n",
    "    \n",
    "    if weighted: \n",
    "        weights = np.power(ratio_per_level['nobs_rel'], 1./3)\n",
    "    else: \n",
    "        weights = 1\n",
    "        \n",
    "    ratio_per_level['diff_to_target_abs'] = abs(ratio_per_level['diff_to_target']) * weights\n",
    "    \n",
    "    #print(ratio_per_level)\n",
    "\n",
    "    ratio_per_level.sort_values('diff_to_target_abs', ascending=False, inplace=True)\n",
    "    ratio_per_level['diff_to_target_abs_cumsum'] = ratio_per_level.diff_to_target_abs.cumsum()\n",
    "    ratio_per_level\n",
    "\n",
    "\n",
    "    x = ratio_per_level['diff_to_target_abs_cumsum'].values.reshape(-1, 1) #df.values #returns a numpy array\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    ratio_per_level['diff_to_target_abs_cumsum_scaled'] = min_max_scaler.fit_transform(x)\n",
    "\n",
    "    print('TargetClass1_ratio', target_ratio)\n",
    "    diff_to_target_df = ratio_per_level.copy()[['diff_to_target_dir', 'diff_to_target_abs']]\n",
    "    diff_to_target_df.diff_to_target_abs = round(diff_to_target_df.diff_to_target_abs, 4)\n",
    "    print(diff_to_target_df)\n",
    "    if saveplots:\n",
    "        diff_to_target_df.to_excel(os.path.join(explorations_path, 'diff_to_target_df.xlsx'))\n",
    "\n",
    "    \n",
    "    ratio_per_level.index.set_names(base_col, inplace=True)\n",
    "    return ratio_per_level\n",
    "\n",
    "\n",
    "def plot_feature_imp_by_target_ratio(data, base_col, weighted=False, saveplots=False): \n",
    "\n",
    "    r = get_feature_imp_by_target_ratio(data, base_col, weighted, saveplots=saveplots)\n",
    "\n",
    "    sns.lineplot(data=r, y=r.index, x='diff_to_target_abs_cumsum_scaled')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if saveplots: \n",
    "        plt.savefig(os.path.join(explorations_path, 'feature_imp_by_target_ratio.png'), dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_feature_imp_by_tree(data, base_col, n_feature=n_feature, saveplots=False): \n",
    "    # prepare\n",
    "    onehot = OneHotEncoder()\n",
    "    X_train_cat = data.loc[:,[base_col]]\n",
    "    #X_train_cat = data[base_col]\n",
    "\n",
    "    X_train_onehot = onehot.fit_transform(X_train_cat)\n",
    "    X_train_onehot_df = pd.DataFrame(X_train_onehot.toarray(), columns=onehot.get_feature_names())\n",
    "    X_train_onehot_df\n",
    "\n",
    "    X_train_onehot_df = pd.get_dummies(data[base_col], prefix=base_col)\n",
    "\n",
    "    # train\n",
    "    dt_gini = DecisionTreeClassifier(random_state = 1)\n",
    "    X_train = X_train_onehot_df#.drop(columns=['x0_Africa','x0_Europe', 'x0_Oceania'])\n",
    "    y_train = data.Income\n",
    "\n",
    "\n",
    "    dt_gini.fit(X_train, y_train) # data[base_col]\n",
    "    print('Score:', dt_gini.score(X_train, y_train))\n",
    "\n",
    "    #dt_gini.feature_importances_\n",
    "    #tree.plot_tree(dt_gini)\n",
    "\n",
    "    #plt.barh(onehot.get_feature_names(), dt_gini.feature_importances_)\n",
    "\n",
    "    #print(dt_gini.feature_importances_)\n",
    "    sorted_idx = dt_gini.feature_importances_.argsort()#[0:10]\n",
    "    plotdata = pd.DataFrame({\n",
    "        'Feature': X_train.columns[sorted_idx], \n",
    "        'Importance': dt_gini.feature_importances_[sorted_idx]}).sort_values('Importance', ascending=False)\n",
    "    #plt.barh()\n",
    "    #print(plotdata)\n",
    "    sns.barplot(data=plotdata.iloc[0:n_feature,:], x='Importance', y='Feature')\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if saveplots: \n",
    "        plt.savefig(os.path.join(explorations_path, 'feature_imp_by_tree.png'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_feature_imp(data, base_col, force_barplot=True, weighted=False, saveplots=False): \n",
    "    print('Class distributions')\n",
    "    if (data[base_col].nunique() < 6) | force_barplot:\n",
    "        n_plots = 2\n",
    "        plot_feature_imp_by_expl(data, base_col, saveplots=saveplots)\n",
    "    else: \n",
    "        n_plots = 1\n",
    "        \n",
    "    print('\\nElbow')        \n",
    "    plot_feature_imp_by_target_ratio(data, base_col, weighted, saveplots=saveplots)\n",
    "    \n",
    "    print('\\nDecision Tree')\n",
    "    plot_feature_imp_by_tree(data, base_col, saveplots=saveplots)\n",
    "    \n",
    "def levels_equal(data, base_col):\n",
    "    train_levels = np.sort(data.loc[~data.Income.isna(), base_col].unique()).tolist()\n",
    "    test_levels = np.sort(data.loc[data.Income.isna(), base_col].unique()).tolist()\n",
    "    \n",
    "    equal = train_levels == test_levels\n",
    "    if not equal: \n",
    "        test_levels_df = pd.DataFrame({\n",
    "            'test_levels': test_levels\n",
    "        })\n",
    "\n",
    "        train_levels_df = pd.DataFrame({\n",
    "            'train_levels': train_levels\n",
    "        })\n",
    "\n",
    "        compare = pd.merge(left=train_levels_df, right=test_levels_df, how='outer', left_on='train_levels', right_on='test_levels')\n",
    "        print(compare.loc[(compare.train_levels.isna()) | (compare.test_levels.isna()) ])\n",
    "\n",
    "        print(f'Levels of \"{base_col}\" differ between test and train set')\n",
    "        #raise ValueError(f'Levels of \"{base_col}\" differ between test and train set')\n",
    "    else: \n",
    "        print('Levels ok')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init \n",
    "cols_to_drop = []\n",
    "cols_to_onehot = []\n",
    "cols_numeric = []\n",
    "cols_bool = []\n",
    "\n",
    "\n",
    "\n",
    "# prep\n",
    "pred_config = {\n",
    "    'cardinality': 'original', # low, medium, high, original\n",
    "    'rmoutlier': True\n",
    "} \n",
    "\n",
    "cardinality = pred_config['cardinality']\n",
    "print('cardinality:', cardinality)\n",
    "\n",
    "error_log = {'cleaning': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract gender from name?!\n",
    "base_col = 'Name'\n",
    "target_col = 'male'\n",
    "\n",
    "salutation = data[base_col].str.split(' ', n=1, expand=True)[0]\n",
    "if salutation.nunique() != 3: \n",
    "    raise ValueError('Unexpected levels of salutation')\n",
    "    \n",
    "print(salutation.value_counts())\n",
    "\n",
    "#gender = ['male' if s == 'Mr.' else 'female' for s in salutation]\n",
    "#data['gender'] = gender\n",
    "\n",
    "male = [1 if s == 'Mr.' else 0 if s in ['Mrs.', 'Miss'] else np.nan for s in salutation]\n",
    "data[target_col] = male\n",
    "\n",
    "if data.male.isna().sum() > 0: \n",
    "    raise Warning('NAs instroduced')\n",
    "\n",
    "\n",
    "sns.countplot(data=data, hue=data.Income, x=target_col)#.set_title(col)\n",
    "plt.show()\n",
    "    \n",
    "cols_to_drop.append(base_col)\n",
    "cols_bool.append(target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute age from Birthday\n",
    "\n",
    "# clean whitespaces\n",
    "data.Birthday = data.Birthday.str.replace(' ', '')\n",
    "# define date format\n",
    "dob_format = '%B%d,%Y'\n",
    "\n",
    "# transform Birthday to datetime, catching the leap year error \n",
    "\n",
    "## helper fct to subtract one day from datetime if error occurs\n",
    "def subone(obj):\n",
    "    val = int(obj.group(0))\n",
    "    return str(val-1)\n",
    "\n",
    "## init and loop over dates\n",
    "dob = []\n",
    "warn_log = []\n",
    "for i, d in enumerate(data.Birthday): \n",
    "    try: \n",
    "        dob.append(datetime.strptime(d, dob_format).date())\n",
    "\n",
    "    except ValueError as e: \n",
    "        if str(e) == 'day is out of range for month': \n",
    "            dt = datetime.strptime(re.sub('\\d{1,2}', subone, d, count=1), dob_format).date()\n",
    "            warn_log.append((d, dt))\n",
    "            dob.append(dt)\n",
    "        else: \n",
    "            raise NotImplementedError('Do not know how to deal with that error!')\n",
    "            dt = np.nan\n",
    "            warn_log.append((d, dt))\n",
    "            dob.append(dt)\n",
    "        \n",
    "# add age column \n",
    "data['age'] = [np.floor((datetime.strptime('2048-12-31', '%Y-%m-%d').date() - d).days / 365.2425) for d in dob]\n",
    "\n",
    "sns.histplot(data, x='age')\n",
    "plt.show()\n",
    "print('Min age:' , min(data.age))\n",
    "\n",
    "# drop date col \n",
    "cols_to_drop.append('Birthday')\n",
    "cols_numeric.append('age')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[~data.Income.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'Native Continent' to bin \n",
    "base_col = 'Native Continent'\n",
    "#sns.countplot(data=data, hue=data.Income, x=base_col)#.set_title(col)\n",
    "#plt.show()\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False, saveplots=False)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality in ['low', 'medium']:\n",
    "        target_col = 'from_europe_or_asia'\n",
    "        #data['from_europe'] = [1 if a == 'Europe' else 0 for a in data[base_col]]\n",
    "        data[target_col] = [1 if a in ['Europe', 'Asia'] else 0 for a in data[base_col]]\n",
    "        cols_bool.append(target_col)\n",
    "    elif cardinality == 'original':\n",
    "        target_col = 'native_continent'\n",
    "        data[target_col] = data[base_col]\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Marital Status\n",
    "base_col = 'Marital Status'\n",
    "#target_col = 'marital_status'\n",
    "\n",
    "data[base_col].value_counts()\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "try: \n",
    "    if cardinality == 'low': \n",
    "        target_col = 'maritalStatus_married'\n",
    "        data[target_col] = [1 if a in ['Married', 'Married - Spouse in the Army'] else 0 for a in data[base_col]]\n",
    "        cols_bool.append(target_col)\n",
    "        \n",
    "    elif cardinality == 'medium': \n",
    "        target_col = 'maritalStatus'\n",
    "        mapping = {\n",
    "            'Married':'Married',\n",
    "            'Single':'Single',\n",
    "            'Divorced':'Divorced',\n",
    "            'Separated':'Separated',\n",
    "            'Widow':'Widow',\n",
    "            'Married - Spouse Missing':'SpouseMissing',\n",
    "            'Married - Spouse in the Army':'Married'\n",
    "        }\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "        \n",
    "    elif cardinality == 'original': \n",
    "        target_col = 'maritalStatus'\n",
    "        mapping = {\n",
    "            'Married':'Married',\n",
    "            'Single':'Single',\n",
    "            'Divorced':'Divorced',\n",
    "            'Separated':'Separated',\n",
    "            'Widow':'Widow',\n",
    "            'Married - Spouse Missing':'SpouseMissing',\n",
    "            'Married - Spouse in the Army':'MarriedArmy'\n",
    "        }\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "    \n",
    "#sns.countplot(data=data, x=target_col)\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lives with\n",
    "base_col = 'Lives with'\n",
    "print(data[base_col].value_counts())\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False, saveplots=True)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "try: \n",
    "    if(cardinality == 'low'): \n",
    "        target_col = 'household_livesWithPartner'\n",
    "        data[target_col] = [1 if a in ['Wife', 'Husband'] else 0 for a in data[base_col]]\n",
    "        cols_bool.append(target_col)\n",
    "    elif(cardinality == 'medium'): \n",
    "        target_col = 'household'\n",
    "        mapping = {\n",
    "            'Wife': 'Partner',\n",
    "            'Other Family': 'Family',\n",
    "            'Children': 'Children',\n",
    "            'Alone': 'Alone',\n",
    "            'Husband': 'Partner',\n",
    "            'Other relatives': 'Family'\n",
    "        }\n",
    "\n",
    "        print(mapping)\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "    elif(cardinality == 'original'): \n",
    "        target_col = 'household'\n",
    "        mapping = {\n",
    "            'Wife': 'Wife',\n",
    "            'Other Family': 'Family',\n",
    "            'Children': 'Children',\n",
    "            'Alone': 'Alone',\n",
    "            'Husband': 'Husband',\n",
    "            'Other relatives': 'Other'\n",
    "        }\n",
    "\n",
    "        print(mapping)\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'Base Area' to bin \n",
    "base_col = 'Base Area'\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "try: \n",
    "    if cardinality == 'low': \n",
    "        target_col = 'basearea_fanfoss' # basearea_northbury\n",
    "        target_val = 'Fanfoss'\n",
    "        target_col_alt = 'basearea_northbury'\n",
    "        target_val_alt = 'Northbury'\n",
    "\n",
    "        print('\\nResult:')\n",
    "        data[target_col] = [1 if a == target_val else 0 for a in data[base_col]]\n",
    "\n",
    "        print('\\nAlternative result:')\n",
    "        test = data[['Income', base_col]].copy()\n",
    "        test[target_col_alt] = [1 if a == target_val_alt else 0 for a in test[base_col]]\n",
    "        sns.countplot(data=test, x=target_col_alt, hue='Income')\n",
    "        plt.show()\n",
    "        cols_bool.append(target_col)\n",
    "    elif cardinality == 'medium':\n",
    "        target_col = 'basearea'\n",
    "        data[target_col] = [\n",
    "            target_val if a == target_val \n",
    "            else target_val_alt if a == target_val_alt \n",
    "            else 'Rest' for a in data[base_col]]\n",
    "        cols_to_onehot.append(target_col)\n",
    "\n",
    "    elif cardinality == 'original':\n",
    "        target_col = 'basearea'\n",
    "        data[target_col] = data[base_col]\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "    \n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Education Level \n",
    "base_col = 'Education Level'\n",
    "target_col = 'education'\n",
    "print(data.columns)\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "edu_mapping = pd.read_csv(os.path.join(src_path, 'edu_mapping.csv'), sep=';')\n",
    "mapping_options = ['level_0', 'level_1', 'numeric', 'original', 'low']\n",
    "\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'low':\n",
    "        m_option = mapping_options[4]\n",
    "        plot_fct = sns.countplot\n",
    "    elif cardinality == 'medium': \n",
    "        m_option = mapping_options[4]\n",
    "        plot_fct = sns.countplot\n",
    "    elif cardinality == 'high': \n",
    "        m_option = mapping_options[2]\n",
    "        plot_fct = sns.histplot\n",
    "    elif cardinality == 'original':\n",
    "        m_option = mapping_options[3]\n",
    "        plot_fct = sns.countplot       \n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "\n",
    "#print(data[base_col].value_counts())\n",
    "\n",
    "#mapping = dict(edu_mapping[['name', mapping_options[2]]].set_index('name'))\n",
    "#mapping = {k:v for k,v in edu_mapping[['name', mapping_options[2]]].set_index('name').items()}\n",
    "#mapping = edu_mapping[['name', mapping_options[2]]].set_index('name')\n",
    "mapping = edu_mapping[['name', m_option]].rename(columns={m_option: target_col})\n",
    "print(mapping)\n",
    "\n",
    "# drop if reruning the cell \n",
    "if target_col in data.columns: \n",
    "    data.drop(columns=[target_col], inplace=True)\n",
    "\n",
    "data = data.merge(mapping, left_on=base_col, right_on='name', how='left')\n",
    "data.drop(columns=['name'], inplace=True)  \n",
    "\n",
    "# plot target col against prediction classes\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "plot_fct(data=data, x=target_col, hue='Income', ax=ax)\n",
    "#plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)\n",
    "\n",
    "\n",
    "data[[base_col, target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years of education \n",
    "base_col = 'Years of Education'\n",
    "target_col = 'education_years'\n",
    "#data.rename(columns={base_col: target_col}, inplace=True)\n",
    "data[target_col] = data[base_col]\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_numeric.append(target_col)\n",
    "data.head()\n",
    "sns.histplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Employment Sector\n",
    "base_col = 'Employment Sector'\n",
    "target_col = 'empl_sector'\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False)\n",
    "#levels_equal(data, base_col)\n",
    "\n",
    "print(data[base_col].value_counts())\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'deprecated':\n",
    "        mapping = {\n",
    "            'Private Sector - Services ': 'private',\n",
    "            'Self-Employed (Individual)': 'self',\n",
    "            'Public Sector - Others': 'public',\n",
    "            '?': 'unknown',\n",
    "            'Private Sector - Others': 'private',\n",
    "            'Self-Employed (Company)': 'self',\n",
    "            'Public Sector - Government': 'public',\n",
    "            'Unemployed': 'delete',\n",
    "            'Never Worked': 'delete'\n",
    "            }\n",
    "\n",
    "    elif cardinality in ['low', 'medium', 'original']: \n",
    "        mapping = {\n",
    "            'Private Sector - Services ': 'private_services',\n",
    "            'Self-Employed (Individual)': 'self_individual',\n",
    "            'Public Sector - Others': 'public_others',\n",
    "            '?': 'unknown',\n",
    "            'Private Sector - Others': 'private_others',\n",
    "            'Self-Employed (Company)': 'self_company',\n",
    "            'Public Sector - Government': 'public_gov',\n",
    "            'Unemployed': 'unemployed',\n",
    "            'Never Worked': 'unemployed'\n",
    "            }\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "\n",
    "print(mapping)\n",
    "    \n",
    "data[target_col] = data[base_col].map(mapping)\n",
    "levels_equal(data, target_col)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "sns.countplot(data=data, x=target_col, hue='Income', ax=ax)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# role\n",
    "base_col = 'Role'\n",
    "target_col = 'empl_role'\n",
    "\n",
    "plot_feature_imp(data.loc[~data.Income.isna()], base_col, weighted=False)\n",
    "levels_equal(data, base_col)\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality in ['low', 'medium']:\n",
    "        mapping = {\n",
    "            'Professor': 'Professor',\n",
    "            'Management': 'Management',\n",
    "            'Repair & constructions': 'Operational_low',\n",
    "            'Administratives': 'Operational',\n",
    "            'Sales': 'Sales',\n",
    "            'Other services': 'Services',\n",
    "            'Machine Operators & Inspectors': 'Operational',\n",
    "            '?': 'unknown',\n",
    "            'Transports': 'Operational_low',\n",
    "            'Cleaners & Handlers': 'Cleaners',\n",
    "            'Agriculture and Fishing': 'Operational',\n",
    "            'IT': 'IT_Security',\n",
    "            'Security': 'IT_Security',\n",
    "            'Household Services': 'Household',\n",
    "            'Army': 'Operational_low'\n",
    "        }\n",
    "    elif cardinality == 'original':       \n",
    "        mapping = {\n",
    "            'Professor': 'Professor',\n",
    "            'Management': 'Management',\n",
    "            'Repair & constructions': 'Constructions',\n",
    "            'Administratives': 'Administratives',\n",
    "            'Sales': 'Sales',\n",
    "            'Other services': 'Services',\n",
    "            'Machine Operators & Inspectors': 'Operator',\n",
    "            '?': 'unknown',\n",
    "            'Transports': 'Transports',\n",
    "            'Cleaners & Handlers': 'Cleaners',\n",
    "            'Agriculture and Fishing': 'Agriculture',\n",
    "            'IT': 'IT', \n",
    "            'Security': 'Security',\n",
    "            'Household Services': 'Household',\n",
    "            'Army': 'Army'\n",
    "        }\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "print(data[base_col].value_counts())\n",
    "\n",
    "print(mapping)\n",
    "    \n",
    "data[target_col] = data[base_col].map(mapping)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "sns.countplot(data=data, x=target_col, hue='Income', ax=ax)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Working Hours per week\n",
    "base_col = 'Working Hours per week'\n",
    "target_col = 'working_hrs_week'\n",
    "\n",
    "data[target_col] = data[base_col]\n",
    "#data.rename(columns={base_col: target_col}, inplace=True)\n",
    "\n",
    "#setting working hours per week of those that have never worked or are unemployed to 0\n",
    "data.loc[data['empl_sector'] == 'unemployed', 'working_hrs_week'] = 0\n",
    "\n",
    "sns.histplot(data=data, x=target_col, hue='Income', bins=30)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_numeric.append(target_col)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money Received\n",
    "base_col = 'Money Received'\n",
    "target_col = 'group_b_received_money'\n",
    "\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'deprecated':\n",
    "        data[target_col] = [1 if v != 0 else 0 for v in data[base_col]]\n",
    "        plot_fct = sns.countplot\n",
    "        cols_bool.append(target_col)\n",
    "    elif cardinality in ['original', 'low']:\n",
    "        data[target_col] = data[base_col]\n",
    "        plot_fct = sns.histplot\n",
    "        cols_numeric.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n",
    "\n",
    "plot_fct(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "#data[[base_col, target_col]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "# Ticket Price\n",
    "base_col = 'Ticket Price'\n",
    "target_col = 'group_c_payed'\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'deprecated':\n",
    "        data[target_col] = [1 if v != 0 else 0 for v in data[base_col]]\n",
    "        sns.countplot(data=data, x=target_col, hue='Income')\n",
    "        cols_bool.append(target_col)\n",
    "    elif cardinality in ['original', 'low']:\n",
    "        data[target_col] = data[base_col]\n",
    "        sns.histplot(data=data, x=target_col, hue='Income', bins = 30)\n",
    "        cols_numeric.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#data[[base_col, target_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_config['rmoutlier'] :\n",
    "    \n",
    "    columns_to_treat = ['education_years' , 'age' , 'working_hrs_week']\n",
    "    \n",
    "    for base_col in columns_to_treat:    \n",
    "        q25 = data[data.Income.notnull()][base_col].quantile(.25)\n",
    "        q75 = data[data.Income.notnull()][base_col].quantile(.75)\n",
    "        iqr = (q75 - q25)\n",
    "        \n",
    "        if base_col in ['education_years' , 'age']:\n",
    "            upper_lim = q75 + 1.5 * iqr\n",
    "            lower_lim = q25 - 1.5 * iqr\n",
    "        else:\n",
    "            upper_lim = q75 + 5.5 * iqr\n",
    "            lower_lim = q25 - 5.5 * iqr\n",
    "     \n",
    "        \n",
    "        if base_col == 'working_hrs_week':\n",
    "            data[base_col] = data[base_col].apply(lambda x: np.nan if x < lower_lim else np.nan if x > upper_lim else x )\n",
    "            # use only train data to calculate the mean\n",
    "            m = data[data.Income.notnull()].groupby('empl_sector').mean()[base_col]\n",
    "            data[base_col] = data.apply(lambda row: m[row['empl_sector']] if pd.isnull(row[base_col]) else row[base_col], axis=1)\n",
    "            #Unemployed and Never worked when getting the mean where being setted to NaN. Added the next step to fill with 0 again.\n",
    "            data[base_col] = data[base_col].fillna(0)\n",
    "        else:\n",
    "            # replace with lower or upper limit accordingly\n",
    "            data[base_col] = data[base_col].apply(lambda x: int(lower_lim) if x < lower_lim else int(upper_lim) if x > upper_lim else x )     \n",
    "              \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "for name, log in error_log.items():\n",
    "    if len(log) > 0: \n",
    "        print(f'{name}:\\n {log}')\n",
    "        raise Warning('Errors occured! See above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "#cols_to_drop.append('CITIZEN_ID')\n",
    "col_with_index = ['CITIZEN_ID']\n",
    "data.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = data.copy().set_index(col_with_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## profile report\n",
    "\n",
    "create_cleaning_report = False\n",
    "if create_cleaning_report: \n",
    "    profile = ProfileReport(\n",
    "        data,\n",
    "        title=f'Cleaned data {dataset_name}' ,\n",
    "        minimal=False, \n",
    "        correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "        }\n",
    "    )\n",
    "    profile.to_file(os.path.join(explorations_path, f'profile_data_cleaned_{dataset_name}.html'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = combined_data.copy().loc[combined_data.Income.isna()]\n",
    "test_data.drop(columns='Income', inplace=True)\n",
    "test_data.info()\n",
    "\n",
    "data = combined_data.copy().loc[~combined_data.Income.isna()]\n",
    "data.Income = data.Income.astype(int)\n",
    "data.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explorations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape[1])\n",
    "data.shape[1] - (len(cols_numeric) + len(cols_bool) + len(cols_to_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check colmuns\n",
    "expected_ncols = len(cols_numeric) + len(cols_bool)\n",
    "for cat_col in cols_to_onehot: \n",
    "    expected_ncols += data[cat_col].nunique()\n",
    "    \n",
    "expected_ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cols_to_drop))\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target distribution\n",
    "\n",
    "sns.countplot(data=data, x='Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_explorations: \n",
    "    # Prepare figure\n",
    "    my_dpi = 200\n",
    "    fig = plt.figure(\n",
    "        figsize=(\n",
    "            #10, 8\n",
    "            1000/my_dpi, 1000/my_dpi\n",
    "        )\n",
    "    ) \n",
    "\n",
    "    # Obtain correlation matrix. Round the values to 2 decimal cases. Use the DataFrame corr() and round() method.\n",
    "    corr = np.round(data.corr(method=\"pearson\"), decimals=2)\n",
    "\n",
    "    # Build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "    mask_annot = np.absolute(corr.values) >= 0.5\n",
    "    annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "    # Plot heatmap of the correlation matrix\n",
    "    sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "                fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "    # Layout\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    # fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(explorations_path, 'correlation_matrix.png')\n",
    "        ,dpi=my_dpi\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions \n",
    "if plot_explorations: \n",
    "    plotdata = data.loc[:,cols_numeric + cols_bool]\n",
    "    ncols = 3\n",
    "    n_plots = plotdata.shape[1]\n",
    "    nrows = int(np.ceil(n_plots/ncols))\n",
    "\n",
    "\n",
    "    my_dpi = 200\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        ncols=ncols, nrows=nrows, \n",
    "        figsize=(\n",
    "            # 15,13\n",
    "            1200/my_dpi, 1000/my_dpi\n",
    "        )\n",
    "    )\n",
    "    col_no = 0\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols): \n",
    "            if col_no < n_plots:\n",
    "                col = plotdata.columns[col_no]\n",
    "                print(col)\n",
    "                if data[col].dtype in [np.float, np.int]: \n",
    "                    sns.histplot(data=plotdata, hue=data.Income, x=col, ax=ax[i,j], bins=30).set_title(col)\n",
    "                else : \n",
    "                    sns.countplot(data=plotdata, hue=data.Income, x=col, ax=ax[i,j], dodge=True).set_title(col)\n",
    "                ax[i,j].tick_params(labelrotation=45)\n",
    "                col_no +=1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(explorations_path, 'distributions.png'), dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep config\n",
    "\n",
    "prep_config = {\n",
    "    'overSampling': False, \n",
    "    'scale': 'standard', # standard, minmax,\n",
    "    'featureselection': False #['boruta', False]\n",
    "   # ,'stratify': 'y' # ['y', 'None']\n",
    "}\n",
    "\n",
    "prep_config\n",
    "\n",
    "# validate config\n",
    "\n",
    "if type(prep_config['overSampling']) is not bool: \n",
    "    raise ValueError()\n",
    "\n",
    "if prep_config['scale'] not in ['standard', 'minmax']: \n",
    "    raise ValueError()\n",
    "    \n",
    "if prep_config['featureselection'] not in ['boruta', False]: \n",
    "    raise ValueError()\n",
    "    \n",
    "    \n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "\n",
    "config_all = merge_two_dicts(pred_config, prep_config)\n",
    "\n",
    "\n",
    "config_str = '_'.join([f'{k}{str(v).capitalize()}' for k, v in config_all.items()])\n",
    "print('config_str: ', config_str)\n",
    "\n",
    "config_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the cleaned data into test and train data \n",
    "X_train = data.copy().drop(['Income'], axis=1)#.values\n",
    "y = data.copy().loc[:,'Income'].values\n",
    "\n",
    "X_test = test_data.copy()#.values\n",
    "X_test.shape\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('X_test.shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366\n",
    "\n",
    "def onehot_scale(X_train, X_test, scaler, verbose=False): \n",
    "    ###Creating series for categorical test and train\n",
    "    X_train_bool = X_train[cols_bool]\n",
    "    X_test_bool = X_test[cols_bool]\n",
    "\n",
    "    ###Instantiating One Hot Encoder\n",
    "    ohe = OneHotEncoder()\n",
    "    ###Creating series for categorical test and train\n",
    "    X_train_cat = X_train[cols_to_onehot]\n",
    "    X_test_cat = X_test[cols_to_onehot]\n",
    "    ###Fitting encoder to training categorical features and transforming ###test and train\n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "    ###Converting series to dataframes\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    X_train_processed = pd.DataFrame(X_train_ohe.todense(), columns=columns, index=X_train_bool.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_ohe.todense(), columns=columns, index=X_test_bool.index)\n",
    "\n",
    "\n",
    "    ###Instantiating Standard Scaler\n",
    "    if scaler == 'standard':        \n",
    "        ss = StandardScaler()\n",
    "    elif scaler == 'minmax': \n",
    "        ss = MinMaxScaler()\n",
    "    else: \n",
    "        raise ValueError(f'Can not interpret {scaler} as scaler!')\n",
    "\n",
    "    ###Converting continuous feature values to floats\n",
    "    X_train_cont = X_train[cols_numeric].astype(float)\n",
    "    X_test_cont = X_test[cols_numeric].astype(float)\n",
    "    ###Fitting scaler to training continuous features and transforming ###train and test\n",
    "    X_train_scaled = ss.fit_transform(X_train_cont)\n",
    "    X_test_scaled = ss.transform(X_test_cont)\n",
    "\n",
    "\n",
    "    ###Concatenating scaled and encoded dataframes\n",
    "    X = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_train_scaled, index=X_train_bool.index, columns=X_train_cont.columns), \n",
    "            X_train_bool, \n",
    "            X_train_processed\n",
    "        ], axis=1\n",
    "    )\n",
    "    X_features = X.columns.to_list()\n",
    "    X = X.values\n",
    "    X_test_ = pd.concat([pd.DataFrame(X_test_scaled, index=X_test_bool.index),X_test_bool, X_test_processed], axis=1).values\n",
    "    \n",
    "    if verbose: \n",
    "        print('X_train_scaled.shape', X_train_scaled.shape)\n",
    "        print('X_train_bool.shape', X_train_bool.shape)\n",
    "        print('X_train_processed.shape', X_train_processed.shape)\n",
    "\n",
    "        print('X.shape', X.shape)\n",
    "        print('y.shape', y.shape)\n",
    "        print('X_test.shape', X_test_.shape)\n",
    "    \n",
    "    return X, X_test_, X_features\n",
    "\n",
    "if True: \n",
    "    X, X_test_, X_features = onehot_scale(X_train, X_test, scaler=prep_config['scale'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featureselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/boruta_py\n",
    "# https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366\n",
    "\n",
    "\n",
    "# load X and y\n",
    "# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n",
    "# X = pd.read_csv('examples/test_X.csv', index_col=0).values\n",
    "# y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n",
    "# y = y.ravel()\n",
    "\n",
    "if prep_config['featureselection'] == 'boruta': \n",
    "\n",
    "    print('X.shape:', X.shape)\n",
    "\n",
    "    # define random forest classifier, with utilising all cores and\n",
    "    # sampling in proportion to y labels\n",
    "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "    # find all relevant features - 5 features should be selected\n",
    "    feat_selector.fit(X, y)\n",
    "\n",
    "    # check selected features - first 5 features are selected\n",
    "    print(feat_selector.support_)\n",
    "    # check ranking of features\n",
    "    print(feat_selector.ranking_)\n",
    "\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    X = feat_selector.transform(X)\n",
    "    X_test_ = feat_selector.transform(X_test_)\n",
    "    \n",
    "elif prep_config['featureselection'] == False: \n",
    "    pass\n",
    "else: \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prep_config['featureselection'] == 'boruta': \n",
    "    \n",
    "    X_metadata = pd.DataFrame({\n",
    "    'Features': (X_features),\n",
    "    'support': (feat_selector.support_), \n",
    "    'ranking': (feat_selector.ranking_)\n",
    "    })\n",
    "\n",
    "    \n",
    "    print(X_metadata.support.sum())\n",
    "    print(X_metadata)\n",
    "    \n",
    "    featselector_filename = f'{config_str}_featselector.csv'\n",
    "    X_metadata.to_csv(os.path.join(comparison_path, featselector_filename), sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overSampling to cope with class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "if prep_config['overSampling']: \n",
    "    sm = SMOTE(random_state=2, n_jobs=-1, k_neighbors=5, sampling_strategy='auto')\n",
    "    X, y = sm.fit_sample(X, y)\n",
    "    sns.countplot(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    if prep_config['stratify'] == 'None': \n",
    "        stratify = None\n",
    "    elif prep_config['stratify']:\n",
    "        stratify = globals()[prep_config['stratify']]\n",
    "\n",
    "    stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test split on updated X\n",
    "X_t, X_val, y_t, y_val = train_test_split(\n",
    "    X, y, random_state=42, \n",
    "    stratify=y, #stratify, \n",
    "    test_size=0.25\n",
    ")\n",
    "sns.countplot(y_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wich models to run\n",
    "modelling_config = {\n",
    "    'naive_bayes1': False,\n",
    "    'naive_bayes2': False,\n",
    "    'naive_bayes3': False,\n",
    "    'logistic_regression_1': False,\n",
    "    'logistic_regression_gs': False,\n",
    "    'logistic_regression_after_gs': False ,\n",
    "    'logistic_regression_rs' : False,\n",
    "    'logistic_regression_after_rs' : False,\n",
    "    'knn_1': False,\n",
    "    'knn_gs': False,\n",
    "    'knn_after_gs': False,\n",
    "    'bagging_knn': False,\n",
    "    'svc_1': False,\n",
    "    'svc_gs': False,\n",
    "    'svc_after_gs': False,\n",
    "    'dt': False,\n",
    "    'random_forest': False, \n",
    "    'boosted_tree': False, \n",
    "    'boosted_tree_gs': False,\n",
    "    'boosted_tree_after_gs': False,\n",
    "    'boosted_tree_after_gs_complete':False,\n",
    "    'adaboost': False,\n",
    "    'mlp': False,\n",
    "    'mlp_gs': False,\n",
    "    'mlp_after_gs': False, \n",
    "    'ensemble1': False,\n",
    "    'ensemble2': False,\n",
    "    'stacking1': False,\n",
    "    'stacking2' : False,\n",
    "    'stacking3' : True, \n",
    "    'stacking3_complete': True\n",
    "}\n",
    "\n",
    "model_comparison = [\n",
    "    'naive_bayes1',\n",
    "    'naive_bayes2',\n",
    "    'naive_bayes3',   \n",
    "    'logistic_regression_1',\n",
    "    #'logistic_regression_gs',\n",
    "    'logistic_regression_after_gs',\n",
    "    #'logistic_regression_rs',\n",
    "    'logistic_regression_after_rs',\n",
    "    'knn_1',\n",
    "    #'knn_gs',\n",
    "    'knn_after_gs',\n",
    "    'bagging_knn',\n",
    "    'svc_1',\n",
    "    #'svc_gs',\n",
    "    'svc_after_gs',\n",
    "    'dt',\n",
    "    'random_forest', \n",
    "    'boosted_tree', \n",
    "    #'boosted_tree_gs',\n",
    "    'boosted_tree_after_gs',\n",
    "    'adaboost',\n",
    "    'mlp',\n",
    "    #'mlp_gs',\n",
    "    'mlp_after_gs', \n",
    "    'ensemble1',\n",
    "    'ensemble2',\n",
    "    'stacking1',\n",
    "    'stacking2',\n",
    "    'stacking3'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate = True\n",
    "models_to_run = 'manual'\n",
    "\n",
    "if models_to_run == 'all_but_gsrs': \n",
    "    modelling_config = {m: (True if bool(re.search('_after_gs|_after_rs', m)) else False if bool(re.search('_gs|_rs', m)) else True) for m, v in modelling_config.items()}\n",
    "elif models_to_run == 'none': \n",
    "    modelling_config = {m:False for m in modelling_config.keys()}\n",
    "elif models_to_run == 'manual': \n",
    "    pass\n",
    "else: \n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission_csv(model, X_test_, test_data, file_no): \n",
    "    test_pred = model.predict(X_test_)\n",
    "    submission_df = pd.DataFrame({'Income':test_pred}, index=test_data.index).reset_index()\n",
    "    filename = f'Group26_Version{file_no}.csv'\n",
    "    submission_df.to_csv(os.path.join(submissions_path, filename), index=False)\n",
    "\n",
    "def train_model(model, params, X_t, y_t, X_val, y_val):\n",
    "    clf = model(random_state=0, verbose=False, **params)\n",
    "\n",
    "    clf.fit(X_t, y_t)\n",
    "    predicted = clf.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', clf.score(X_val, y_val))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, params, X_t, y_t, X_val, y_val):\n",
    "    clf = model(**params)\n",
    "\n",
    "    clf.fit(X_t, y_t)\n",
    "    predicted = clf.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', clf.score(X_val, y_val))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Naive Bayes</font> <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Instantiating gaussian naive bayes\n",
    "naive_bayes1 = GaussianNB()\n",
    "\n",
    "if modelling_config['naive_bayes1']: \n",
    "    ###Fitting naive bayes to train\n",
    "    naive_bayes1.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = naive_bayes1.predict(X_val)\n",
    "    ###Training Score\n",
    "    naive_bayes1.score(X_t, y_t)\n",
    "    ###Test Score\n",
    "    naive_bayes1.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.48178571428571426\n",
    "    # f1_micro: 0.44875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Instantiating multinomial naive bayes\n",
    "naive_bayes2 = MultinomialNB()\n",
    "\n",
    "###used data scaled with minmax here \n",
    "if modelling_config['naive_bayes2'] and prep_config['scale'] == 'minmax': \n",
    "\n",
    "    ###Fitting naive bayes to train\n",
    "    naive_bayes2.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = naive_bayes2.predict(X_val)\n",
    "    ###Training Score\n",
    "    naive_bayes2.score(X_t, y_t)\n",
    "    ###Test Score\n",
    "    naive_bayes2.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8044642857142859\n",
    "\n",
    "\n",
    "###Instantiating complement naive bayes\n",
    "naive_bayes3 = ComplementNB()\n",
    "\n",
    "if modelling_config['naive_bayes3']: \n",
    "\n",
    "    ###Fitting naive bayes to train\n",
    "    naive_bayes3.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = naive_bayes3.predict(X_val)\n",
    "    ###Training Score\n",
    "    naive_bayes3.score(X_t, y_t)\n",
    "    ###Test Score\n",
    "    naive_bayes3.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.7673214285714286\n",
    "    \n",
    "    if cross_validate: \n",
    "        scores = cross_val_score(naive_bayes3, X, y, cv=10, scoring='f1_micro')\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        #0.7610267857142856 ~ 0.005905863622498532\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Logistic Regression</font> <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ###Instantiating logistic regression\n",
    "logistic_regression_1 = LogisticRegression()\n",
    "\n",
    "if modelling_config['logistic_regression_1']: \n",
    "   \n",
    "    ###Fitting logistic regression to train\n",
    "    logistic_regression_1.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = logistic_regression_1.predict(X_val)\n",
    "    ###Training Score\n",
    "    logistic_regression_1.score(X_t,y_t)\n",
    "    ###Test Score\n",
    "    logistic_regression_1.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8528571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1w = make_scorer(f1_score, average='micro')\n",
    "\n",
    "parameter_space = {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': list(range(1,10)),\n",
    "        'class_weight':[{0:1.0, 1:1.0},{0:1.0, 1:50.0}],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'multi_class':['auto','ovr','multinomial']\n",
    "        #dual\n",
    "        #tolerance for stopping criteria\n",
    "        #max_iter\n",
    "        #refit\n",
    "        #intercept_scaling\n",
    "        #l1_ratios\n",
    "    }\n",
    "\n",
    "logistic_regression_gs = GridSearchCV( LogisticRegression(random_state=0, verbose=False) , parameter_space, n_jobs=-1, scoring=f1w, cv=5, verbose=10)\n",
    "\n",
    "if modelling_config['logistic_regression_gs']:   \n",
    "\n",
    "    logistic_regression_gss.fit(X_t, y_t)\n",
    "    print(f'{logistic_regression_gs.scoring}: {logistic_regression_gs.best_score_}')\n",
    "    # make_scorer(f1_score, average=micro): 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_gs.best_params_\n",
    "#{'C': 1,\n",
    "# 'class_weight': {0: 1.0, 1: 1.0},\n",
    "# 'multi_class': 'auto',\n",
    "# 'penalty': 'l2',\n",
    "# 'solver': 'liblinear'}\n",
    "\n",
    "#logistic_regression_after_gs = LogisticRegression(C=1,class_weight={0: 1.0, 1: 1.0},penalty = 'l2',solver='liblinear',multi_class='auto')\n",
    "\n",
    "lr_params_short = {\n",
    "        'C': 1,\n",
    "        'class_weight': {0: 1.0, 1: 1.0},\n",
    "        'penalty': 'l2',\n",
    "        'solver': 'liblinear',\n",
    "        'multi_class': 'auto'\n",
    "    }\n",
    "\n",
    "logistic_regression_after_gs = LogisticRegression(random_state=0, verbose=False, **lr_params_short)\n",
    "\n",
    "\n",
    "if modelling_config['logistic_regression_after_gs']:     \n",
    "    # train final model with best_params from grid search \n",
    "   # logistic_regression_after_gs  = train_model(\n",
    "    #    LogisticRegression,\n",
    "   #     lr_params_short,\n",
    "    #   lr_gs.best_params_,\n",
    "    #    X_t, y_t, X_val, y_val\n",
    "   # )\n",
    "    \n",
    "    logistic_regression_after_gs.fit(X_t, y_t)\n",
    "    predicted = logistic_regression_after_gs.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', logistic_regression_after_gs.score(X_val, y_val))\n",
    "    \n",
    "    \n",
    "    # f1_micro: 0.8530357142857142\n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            logistic_regression_after_gs , X, y, cv=10, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8510714285714286 ~ 0.008151937293223808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Randomized Search\n",
    "logistic_regression_rs = RandomizedSearchCV(estimator=LogisticRegression(random_state=0, verbose=False), param_distributions=parameter_space, n_iter=100)\n",
    "if modelling_config['logistic_regression_rs']:     \n",
    "    logistic_regression_rs.fit(X_t, y_t)\n",
    "    \n",
    "#logistic_regression_after_rs = RandomizedSearchCV(estimator=LogisticRegression(random_state=0, verbose=False), param_distributions=parameter_space, n_iter=100)\n",
    "lr_params_short = {\n",
    "        'C': 6,\n",
    "        'class_weight': {0: 1.0, 1: 1.0},\n",
    "        'penalty': 'l2',\n",
    "        'solver': 'saga',\n",
    "        'multi_class': 'ovr'\n",
    "}\n",
    "\n",
    "logistic_regression_after_rs = LogisticRegression(random_state=0, verbose=False, **lr_params_short)\n",
    "\n",
    "if modelling_config['logistic_regression_after_rs']: \n",
    "    #logistic_regression_after_rs.fit(X_t, y_t)\n",
    "    # rsearch.best_score_): 0.8498809523809523\n",
    "    # train final model with best_params from grid search \n",
    "   # lr = train_model(\n",
    "   #     LogisticRegression,\n",
    "   #     lr_params_short,\n",
    "    #   rsearch.best_params_ : {'solver': 'saga','penalty': 'l2','multi_class': 'ovr','class_weight': {0: 1.0, 1: 1.0},'C': 6}\n",
    "   #     X_t, y_t, X_val, y_val\n",
    "  #  )\n",
    "    # f1_micro: 0.8528571428571429\n",
    "    \n",
    "    logistic_regression_after_rs.fit(X_t, y_t)\n",
    "    predicted = logistic_regression_after_rs.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', logistic_regression_after_rs.score(X_val, y_val))\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            logistic_regression_after_rs, X, y, cv=10, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8514285714285714 ~ 0.008329931278350444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>KNN</font> <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Instantiating k nearest neighbors \n",
    "knn_1 = KNeighborsClassifier()\n",
    "\n",
    "if modelling_config['knn_1']: \n",
    "    \n",
    "    ###Fitting k nearnest neighbors to train\n",
    "    knn_1.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = knn_1.predict(X_val)\n",
    "    ###Training Score\n",
    "    knn_1.score(X_t,y_t)\n",
    "    ###Test Score\n",
    "    knn_1.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8351785714285714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1w = make_scorer(f1_score, average='micro')\n",
    "\n",
    "parameter_space = {\n",
    "        'n_neighbors': list(range(1,15)),\n",
    "        'weights': ['uniform','distance'],\n",
    "        'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size' : [5,10,15,30,45] \n",
    "}\n",
    "\n",
    "knn_gs = GridSearchCV( KNeighborsClassifier(), parameter_space, n_jobs=-1, scoring=f1w, cv=5, verbose=10)\n",
    "\n",
    "if modelling_config['knn_gs']: \n",
    "    knn_gs.fit(X_t, y_t)\n",
    "    print(f'{knn_gs.scoring}: {knn_gs.best_score_}')\n",
    "    # make_scorer(f1_score, average=micro): 0.8419047619047619\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_gs.best_params_\n",
    "#{'algorithm': 'brute',\n",
    "# 'leaf_size': 15,\n",
    "# 'n_neighbors': 11,\n",
    "# 'weights': 'uniform'}\n",
    "#knn = KNeighborsClassifier(algorithm = 'brute', n_neighbors = 11, weights = 'uniform')\n",
    "\n",
    "knn_params_short = {\n",
    "        'algorithm': 'brute',\n",
    "        'n_neighbors': 11,\n",
    "        'weights': 'uniform'\n",
    "}\n",
    "knn_after_gs = KNeighborsClassifier(**knn_params_short)\n",
    "\n",
    "if modelling_config['knn_after_gs']:    \n",
    "\n",
    "    # train final model with best_params from grid search \n",
    "   # knn = train_model_simple(\n",
    "    #    KNeighborsClassifier,\n",
    "     #   knn_params_short,\n",
    "    #   knn_gs.best_params_,\n",
    "    #    X_t, y_t, X_val, y_val\n",
    "   # )\n",
    "    \n",
    "    knn_after_gs.fit(X_t, y_t)\n",
    "    predicted = knn_after_gs.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', knn_after_gs.score(X_val, y_val))\n",
    "    # f1_micro: 0.8483928571428572\n",
    "\n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            knn_after_gs, X, y, cv=10, scoring='f1_micro'\n",
    "        )\n",
    "\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8396428571428572 ~ 0.008096988606253283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging knn\n",
    "\n",
    "###Instantiating k nearest neighbors \n",
    "bagging_knn = BaggingClassifier(base_estimator = knn_1, random_state = 0)\n",
    "\n",
    "if modelling_config['bagging_knn']: \n",
    "    \n",
    "    ###Fitting k nearnest neighbors to train\n",
    "    bagging_knn.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = bagging_knn.predict(X_val)\n",
    "    ###Training Score\n",
    "    bagging_knn.score(X_t,y_t)\n",
    "    ###Test Score\n",
    "    bagging_knn.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8469642857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Support Vector Machine</font> <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Instantiating support vector classifier\n",
    "svc_1 = SVC(gamma='scale')\n",
    "\n",
    "if modelling_config['svc_1']: \n",
    "    \n",
    "    ###Fitting logistic regression to train\n",
    "    svc_1.fit(X_t,y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = svc_1.predict(X_val)\n",
    "    ###Training Score\n",
    "    svc_1.score(X_t,y_t)\n",
    "    ###Test Score\n",
    "    svc_1.score(X_val,y_val)\n",
    "    \n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1w = make_scorer(f1_score, average='micro')\n",
    "\n",
    "parameter_space = {\n",
    "        'C': [0.1,0.5,1,10,100],\n",
    "        'kernel': ['linear','poly','rbf','sigmoid'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svc_gs = GridSearchCV( SVC(), parameter_space, n_jobs=-1, scoring=f1w, cv=5, verbose=10)\n",
    "\n",
    "if modelling_config['svc_gs']: \n",
    "    \n",
    "    svc_gs.fit(X_t, y_t)\n",
    "    print(f'{svc_gs.scoring}: {svc_gs.best_score_}')\n",
    "    # make_scorer(f1_score, average=micro): 0.85375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_gs.best_params_\n",
    "#{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "\n",
    "svc_params_short = {\n",
    "        'C': 1,\n",
    "        'gamma': 'scale',\n",
    "        'kernel': 'rbf'\n",
    "}\n",
    "\n",
    "svc_after_gs = SVC(random_state=0, verbose=False, probability = True , **svc_params_short)\n",
    "\n",
    "\n",
    "if modelling_config['svc_after_gs']: \n",
    "    \n",
    "    svc_after_gs.fit(X_t, y_t)\n",
    "    predicted = svc_after_gs.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', svc_after_gs.score(X_val, y_val))\n",
    "\n",
    "\n",
    "    # train final model with best_params from grid search \n",
    "    #svc = train_model(\n",
    "    #    SVC,\n",
    "    #    svc_params_short,\n",
    "    #   svc_gs.best_params_,\n",
    "    #    X_t, y_t, X_val, y_val\n",
    "    # )\n",
    "    # f1_micro:0.8625\n",
    "\n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            svc_after_gs, X, y, cv=10, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8551785714285713 ~ 0.012511474325430974"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Decision Trees</font> <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Instantiating Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "if modelling_config['dt']: \n",
    "    ###Fitting Decision Tree Classifier to train and test\n",
    "    dt.fit(X_t, y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = dt.predict(X_val)\n",
    "    ###Test Score\n",
    "    dt.score(X_val, y_val)\n",
    "    ###Training Score\n",
    "    dt.score(X_t, y_t)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8139285714285716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Random Forest</font> <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_jobs=-1, n_estimators=500, oob_score=True, max_depth=6, random_state=42)\n",
    "\n",
    "if modelling_config['random_forest']: \n",
    "    ###Instantiating Random Forest Classifier\n",
    "    ###Fitting Random Forest Classifier to train and test\n",
    "    random_forest.fit(X_t, y_t)\n",
    "    ###Predicting on test data\n",
    "    predicted = random_forest.predict(X_val)\n",
    "    ###Test Score\n",
    "    random_forest.score(X_val, y_val)\n",
    "    ###Training Score\n",
    "    random_forest.score(X_t, y_t)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8482142857142857\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Boosted Trees</font> <a class=\"anchor\" id=\"seventh-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_tree = GradientBoostingClassifier(\n",
    "    learning_rate=.1,\n",
    "    random_state=0, verbose=False,\n",
    "    subsample=1,\n",
    "    n_estimators=1000, max_depth=7, min_impurity_decrease = 0.1,\n",
    "    max_features='auto',\n",
    "    n_iter_no_change=30, validation_fraction=0.1,\n",
    "    warm_start=False\n",
    ")\n",
    "\n",
    "if modelling_config['boosted_tree']: \n",
    "    boosted_tree.fit(X_t, y_t)\n",
    "    predicted = boosted_tree.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    # f1_micro: 0.8755357142857143\n",
    "    print('Acc:', boosted_tree.score(X_val, y_val))\n",
    "\n",
    "    save_submission_csv(boosted_tree, X_test_, test_data, '12')\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "        boosted_tree, X, y, cv=10, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8708482142857144 ~ 0.006533890504710872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1w = make_scorer(f1_score, average='micro')\n",
    "\n",
    "parameter_space = {\n",
    "        'learning_rate':[.1, .05],\n",
    "        'subsample':[1, .9],\n",
    "        'n_estimators':[1000], \n",
    "        'min_samples_split': [2,3],\n",
    "        'max_depth': list(range(5,9)), \n",
    "        'min_impurity_decrease': [0.0, 0.1],\n",
    "        'max_features': ['auto', None],\n",
    "        'n_iter_no_change': [30],\n",
    "        'validation_fraction': [0.1],\n",
    "        'warm_start': [False], \n",
    "        'ccp_alpha': [0.0]\n",
    "}\n",
    "\n",
    "boosted_tree_gs = GridSearchCV( GradientBoostingClassifier(random_state=0, verbose=False) , parameter_space, n_jobs=-1, scoring=f1w, cv=5, verbose=10)\n",
    "\n",
    "if modelling_config['boosted_tree_gs']: \n",
    "    \n",
    "    boosted_tree_gs.fit(X, y)\n",
    "\n",
    "    print(f'{boosted_tree_gs.scoring}: {boosted_tree_gs.best_score_}')\n",
    "    # make_scorer(f1_score, average=micro): 0.8710267857142856\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf3.get_params()\n",
    "\n",
    "clf3_params_short = {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'max_features': 'auto',\n",
    "        'min_impurity_decrease': 0.1,\n",
    "        'min_samples_split': 3,\n",
    "        'n_estimators': 1000,\n",
    "        'n_iter_no_change': 30,\n",
    "        'subsample': 1,\n",
    "        'validation_fraction': 0.1,\n",
    "        'warm_start': False\n",
    "    }\n",
    "\n",
    "boosted_tree_after_gs = GradientBoostingClassifier(random_state=0, verbose=False, **clf3_params_short)\n",
    "\n",
    "if modelling_config['boosted_tree_after_gs']: \n",
    "    \n",
    "    boosted_tree_after_gs.fit(X_t, y_t)\n",
    "    \n",
    "    predicted = boosted_tree_after_gs.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "    print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "    print('Acc:', boosted_tree_after_gs.score(X_val, y_val))\n",
    "\n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            boosted_tree_after_gs , X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8706249999999999 ~ 0.004483441047058505\n",
    "        \n",
    "    save_submission_csv( boosted_tree_after_gs , X_test_, test_data, '11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelling_config['boosted_tree_after_gs_complete']: \n",
    "    boosted_tree_after_gs.fit(X, y)\n",
    "\n",
    "    # save\n",
    "    save_submission_csv( boosted_tree_after_gs , X_test_, test_data, '18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adaboost = AdaBoostClassifier(random_state = 5)\n",
    "\n",
    "if modelling_config['adaboost']:     \n",
    "\n",
    "    adaboost.fit(X_t, y_t)\n",
    "    predicted = adaboost.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    adaboost.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            adaboost, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8580357142857145 ~ 0.004430677062785573"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Neural Networks</font> <a class=\"anchor\" id=\"eighth-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "            alpha=0.000001,\n",
    "            activation='relu', # [tanh, relu]\n",
    "            random_state=0, verbose=False, \n",
    "            hidden_layer_sizes = (100, ),# [(100,), (50,50), (200,)]\n",
    "            max_iter=1000, early_stopping=True, \n",
    "            learning_rate_init = 0.001,  # [0.05, 0.001, 0.005]\n",
    "            learning_rate='constant', # ['invscaling', 'constant']\n",
    "            momentum=0.9, #[0.8, 0.9]\n",
    "            solver='adam', # ['adma', 'sgd']\n",
    "            beta_1 = .9, #[.7, .8, .9]\n",
    "            beta_2 = .999 #[.9, .99, .8]\n",
    "        )\n",
    "\n",
    "if modelling_config['mlp']: \n",
    "        \n",
    "        mlp.fit(X_t, y_t)\n",
    "        predicted = mlp.predict(X_val)\n",
    "\n",
    "        print(classification_report(y_val, predicted))\n",
    "        print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "        # f1_micro: 0.8614285714285714\n",
    "        print('Acc:', mlp.score(X_val, y_val))\n",
    "        \n",
    "        if cross_validate: \n",
    "                scores = cross_val_score(\n",
    "                mlp, X, y, cv=5, scoring='f1_micro'\n",
    "                )\n",
    "                print(f'{scores.mean()} ~ {scores.std()}')\n",
    "                # 0.8549107142857144 ~ 0.004171181652427915\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1w = make_scorer(f1_score, average='micro')\n",
    "   \n",
    "params = {\n",
    "        'alpha': (10.0 ** -np.arange(1, 7)).tolist(), \n",
    "        'activation': ['tanh', 'relu'],  \n",
    "        'hidden_layer_sizes': [(100,), (50,50), (200,), (50,50, 50 )], \n",
    "        'learning_rate_init': [0.05, 0.001, 0.005],\n",
    "        'learning_rate':['adaptive', 'constant'], \n",
    "        'momentum':[0.8, 0.9], \n",
    "        'solver': ['adam'], #['adma', 'sgd']\n",
    "        'beta_1': [.7, .8, .9], \n",
    "        'beta_2':[.9, .99, .8]\n",
    "}\n",
    "\n",
    "\n",
    "mlp_gs= GridSearchCV( MLPClassifier(random_state=0, verbose=False, max_iter=1000, early_stopping=True) , params, n_jobs=-1, scoring=f1w, cv=5, verbose=10)\n",
    "\n",
    "if modelling_config['mlp_gs']:\n",
    "    mlp_gs.fit(X, y)\n",
    "\n",
    "    print(f'{mlp_gs.scoring}: {mlp_gs.best_score_}')\n",
    "    #make_scorer(f1_score, average=micro): 0.8584375\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_gs_best_params = {\n",
    "    'activation': 'tanh',\n",
    "    'alpha': 0.0001,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.8,\n",
    "    'hidden_layer_sizes': (50, 50, 50),\n",
    "    'learning_rate': 'adaptive',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'momentum': 0.8,\n",
    "    'solver': 'adam'\n",
    "}\n",
    "\n",
    "mlp_after_gs = MLPClassifier(random_state=0, verbose=False, max_iter=1000, early_stopping=True, **mlp_gs_best_params)\n",
    "\n",
    "\n",
    "if modelling_config['mlp_after_gs']: \n",
    "        mlp_after_gs.fit(X_t, y_t)\n",
    "        predicted = mlp_after_gs.predict(X_val)\n",
    "\n",
    "        print(classification_report(y_val, predicted))\n",
    "        print('f1_micro:', f1_score(y_val, predicted, average='micro')) \n",
    "        # f1_micro: 0.8546428571428571\n",
    "        print('Acc:', mlp_after_gs.score(X_val, y_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Ensembles</font> <a class=\"anchor\" id=\"ninth-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble1 = VotingClassifier(\n",
    "    estimators=[('rf', random_forest), ('gbt', boosted_tree), ('mlp', mlp_after_gs)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "if modelling_config['ensemble1']: \n",
    "    ensemble1.fit(X_t, y_t)\n",
    "    predicted = ensemble1.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    ensemble1.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            ensemble1, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        #0.8653125000000002 ~ 0.0036202026665559423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble2 = VotingClassifier(\n",
    "        estimators=[('gbt', boosted_tree), ('svc', svc_1)],\n",
    "        voting='soft')\n",
    "\n",
    "if modelling_config['ensemble2']:     \n",
    "\n",
    "    ensemble2.fit(X_t, y_t)\n",
    "    predicted = ensemble2.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    ensemble2.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            ensemble2, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        #0.8668750000000001 ~ 0.004359484148476345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('gbt', boosted_tree), ('mlp', mlp_after_gs)]\n",
    "stacking1 = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "if modelling_config['stacking1']: \n",
    "    \n",
    "\n",
    "    stacking1.fit(X_t, y_t)\n",
    "    predicted = stacking1.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    stacking1.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            stacking1, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        #0.8653125000000002 ~ 0.0036202026665559423\n",
    "        \n",
    "    save_submission_csv(stacking1, X_test_, test_data, '13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking2 = StackingClassifier(\n",
    "        estimators=[('gbt', boosted_tree), ('mlp', mlp_after_gs)],\n",
    "        final_estimator=svc_1)\n",
    "\n",
    "if modelling_config['stacking2']:     \n",
    "\n",
    "    stacking2.fit(X_t, y_t)\n",
    "    predicted = stacking2.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    stacking2.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            stacking2, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8708482142857144 ~ 0.003944266537698527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking3 = StackingClassifier(\n",
    "    n_jobs=-1,\n",
    "    estimators=[('rf', random_forest), ('gbt', boosted_tree), ('mlp', mlp_after_gs), ('knn', knn_after_gs)],\n",
    "    final_estimator=svc_1)\n",
    "\n",
    "if modelling_config['stacking3']: \n",
    "\n",
    "\n",
    "    stacking3.fit(X_t, y_t)\n",
    "    predicted = stacking3.predict(X_val)\n",
    "\n",
    "    print(classification_report(y_val, predicted))\n",
    "\n",
    "    stacking3.score(X_val, y_val)\n",
    "    \n",
    "    if cross_validate:\n",
    "        scores = cross_val_score(\n",
    "            stacking3, X, y, cv=5, scoring='f1_micro'\n",
    "        )\n",
    "        print(f'{scores.mean()} ~ {scores.std()}')\n",
    "        # 0.8701339285714285 ~ 0.0036306474305977565\n",
    "        # 0.86969 ~ 0.00289 rmoutliers=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelling_config['stacking3_complete']: \n",
    "    stacking3.fit(X, y)\n",
    "\n",
    "    # save\n",
    "    save_submission_csv(stacking3 , X_test_, test_data, '20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare f1 score over classes\n",
    "if False: \n",
    "    if models_to_run != 'none': \n",
    "        raise Warning('For comp mode \"models_to_run\" should be set to \"none\"')\n",
    "\n",
    "    # cross validate models \n",
    "    class_scores = []\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    fold_no = 0\n",
    "    print('***fold_no: ', fold_no)\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_tt, X_vv = X[train_index], X[test_index]\n",
    "        y_tt, y_vv = y[train_index], y[test_index]\n",
    "\n",
    "        for model in model_comparison:#[0:4]:\n",
    "            print(model)\n",
    "            try: \n",
    "                # train model \n",
    "                model_inst = globals()[model]\n",
    "                model_inst.fit(X_tt, y_tt)\n",
    "                predicted = model_inst.predict(X_vv)\n",
    "\n",
    "\n",
    "                # calc scores \n",
    "                f1 = f1_score(y_vv, predicted, average=None).round(2).tolist()\n",
    "                f1_0 = f1[0]\n",
    "                f1_1 = f1[1]\n",
    "                f1_micro = f1_score(y_vv, predicted, average='micro')\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                f1_0 = np.nan\n",
    "                f1_1 = np.nan\n",
    "                f1_micro = np.nan\n",
    "            finally: \n",
    "                class_scores.append((model, f1_0, f1_1, f1_micro, fold_no))\n",
    "                class_scores\n",
    "        fold_no += 1\n",
    "\n",
    "    # build df         \n",
    "    class_scores_per_fold_df = pd.DataFrame(class_scores, columns=['model', 'class0', 'class1', 'micro', 'fold_no'])\n",
    "    class_scores_per_fold_df\n",
    "\n",
    "    classScores_filename = f'{config_str}_classScores.csv'\n",
    "    class_scores_per_fold_df.to_csv(os.path.join(comparison_path, classScores_filename), sep=';', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot F1 score over classes\n",
    "if True:\n",
    "    # prepare plot data \n",
    "    classScores_filename = 'cardinalityOriginal_rmoutlierFalse_overSamplingFalse_scaleStandard_featureselectionFalse_classScores.csv'\n",
    "    class_scores_per_fold_df = pd.read_csv(os.path.join(comparison_path, classScores_filename), sep=';')\n",
    "    print(class_scores_per_fold_df)\n",
    "    \n",
    "    class_scores_df = class_scores_per_fold_df.groupby('model')[['class0', 'class1', 'micro']].mean()\n",
    "\n",
    "    class_scores_df['diff'] = class_scores_df.class0 - class_scores_df.class1\n",
    "    class_scores_df['perc_diff'] = class_scores_df.apply(lambda row: (row.loc['class0']-row.loc['class1'])/row.loc['class0'], axis=1)\n",
    "    class_scores_df.reset_index(inplace=True)\n",
    "    print(class_scores_df)\n",
    "\n",
    "    class_scores_df_wide = pd.melt(class_scores_df, id_vars='model', value_vars=['class0', 'class1'], var_name='class', value_name='f1')\n",
    "    class_scores_df_wide\n",
    "\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "\n",
    "    sns.pointplot(data=class_scores_df_wide, x='model', y='f1', hue='class', ax=ax[0], scale=.5, order=model_comparison)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "    sns.pointplot(data=class_scores_df, x='model', y='perc_diff', color='black', ax=ax[1], scale=.5, order=model_comparison)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.savefig(os.path.join(explorations_path, 'f1_per_class.png'), dpi=200, bbox_inches = \"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all models for the defined preparation config\n",
    "\n",
    "if False: \n",
    "    \n",
    "    # check setup \n",
    "    print('models_to_run:',models_to_run)\n",
    "\n",
    "    if models_to_run != 'none': \n",
    "        raise Warning('For comp mode \"models_to_run\" should be set to \"none\"')\n",
    "    \n",
    "    # cross validate models \n",
    "    overview = []\n",
    "    exceptions = []\n",
    "    \n",
    "    for model in model_comparison:\n",
    "        print(model)\n",
    "        try: \n",
    "            model_inst = globals()[model]\n",
    "            \n",
    "            specs = str(model_inst).replace('\\n', '')\n",
    "            specs = ' '.join(specs.split())\n",
    "\n",
    "            if True: \n",
    "                scores = cross_val_score(\n",
    "                    model_inst, X, y, cv=5, scoring='f1_micro'\n",
    "                )\n",
    "                print(f'{scores.mean()} ~ {scores.std()}')\n",
    "            mean = scores.mean()\n",
    "            sd = scores.std()\n",
    "\n",
    "\n",
    "        except Exception as e: \n",
    "            mean = np.nan\n",
    "            sd = np.nan\n",
    "            specs = ''\n",
    "            exceptions.append((model, str(e)))\n",
    "            \n",
    "        finally:\n",
    "\n",
    "            overview.append((model, mean, sd, specs))\n",
    "\n",
    "    # prepare dfs and save to csv\n",
    "    overview_filename = f'{config_str}_overview.csv'\n",
    "    exceptions_filename = f'{config_str}_exceptions.csv'\n",
    "\n",
    "\n",
    "    overview_df = pd.DataFrame(overview, columns=['model', 'mean', 'sd', 'specs'])\n",
    "    overview_df.to_csv(os.path.join(comparison_path, overview_filename), sep=';')\n",
    "\n",
    "    exceptions_df = pd.DataFrame(exceptions, columns=['model', 'exception'])\n",
    "    exceptions_df.to_csv(os.path.join(comparison_path, exceptions_filename), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scores over all configs and models based on the csv files generated in the cell above\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# collect scores from csv  files\n",
    "\n",
    "if True: \n",
    "    configs = {}\n",
    "    scores = pd.DataFrame()\n",
    "    for i, path in enumerate(os.listdir(comparison_path)):\n",
    "        identifier = '_overview.csv'\n",
    "        if identifier in path:\n",
    "            config_desc = path.replace(identifier, '')\n",
    "            configs[i] = config_desc\n",
    "            tmp = pd.read_csv(os.path.join(comparison_path, path), sep=';')\n",
    "            tmp.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "            tmp['config'] = config_desc\n",
    "            scores = pd.concat([scores, tmp], axis=0)\n",
    "    scores.reset_index(inplace=True)\n",
    "    scores\n",
    "    \n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # plot scores \n",
    "    def plot_scores(scores, explorations_path, filename, width, height, dodge): \n",
    "        my_dpi = 200\n",
    "        fig = plt.figure(\n",
    "            figsize=(\n",
    "                #10, 8\n",
    "                width/my_dpi, height/my_dpi\n",
    "            )\n",
    "        )\n",
    "        ax = sns.pointplot(data=scores, x='model', y='mean', hue='config', alpha=.7, dodge=dodge, join=False, scale=.5)\n",
    "\n",
    "        # Find the x,y coordinates for each point\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        for point_pair in ax.collections:\n",
    "            for x, y in point_pair.get_offsets():\n",
    "                x_coords.append(x)\n",
    "                y_coords.append(y)\n",
    "\n",
    "        # Calculate the type of error to plot as the error bars\n",
    "        # Make sure the order is the same as the points were looped over\n",
    "        #errors = tips.groupby(['smoker', 'sex']).std()['tip']\n",
    "        #colors = ['steelblue']*2 + ['coral']*2\n",
    "        ax.errorbar(x_coords, y_coords, yerr=scores.sd, fmt=' ', zorder=-1, color='black', capsize=2)\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "                   ncol=1, mode=\"expand\", borderaxespad=0., prop={'size': 6})\n",
    "        plt.tight_layout()\n",
    "        plt.xticks(rotation=90)\n",
    "        ax.set(ylabel='mean micro f1 score')\n",
    "\n",
    "\n",
    "        plt.savefig(os.path.join(explorations_path, f'{filename}.png'), dpi=200, bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "       \n",
    "    # generate and save plot\n",
    "    plot_scores(scores, explorations_path, 'comp_all', 1200, 1400, .4)\n",
    "    \n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # save additional infos\n",
    "    best_model_scores = scores[scores.model == 'stacking3'].drop(columns=['specs', 'model', 'index'])\n",
    "    best_model_scores[['mean', 'sd']] = best_model_scores[['mean', 'sd']].apply(lambda x: np.round(x,5))\n",
    "    best_model_scores.to_excel(os.path.join(explorations_path, 'stacking3.xlsx'), index=False)\n",
    "\n",
    "    models_overview =  pd.DataFrame(scores['model'].unique(), columns=['model'])\n",
    "    models_overview.to_excel(os.path.join(explorations_path, 'models_overview.xlsx'), index=False)\n",
    "\n",
    "    configs_overview = pd.DataFrame(scores['config'].unique(), columns=['config'])\n",
    "    configs_overview.to_excel(os.path.join(explorations_path, 'configs_overview.xlsx'), index=False)\n",
    "    \n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # TABLE scores_comp\n",
    "    scores_comp = scores.loc[:, ['model', 'mean', 'sd', 'config']].sort_values('mean', ascending=False)#.head(10)\n",
    "    scores_comp[['mean', 'sd']] = scores_comp[['mean', 'sd']].apply(lambda x: np.round(x,5))\n",
    "    scores_comp['sd/mean'] = scores_comp['sd'] / scores_comp['mean']\n",
    "    scores_comp.to_excel(os.path.join(explorations_path, 'scores_comp.xlsx'), index=False)\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # PLOT stableTop10\n",
    "    stableTop10 = scores_comp.sort_values('sd/mean').head(10)\n",
    "    stableTop10['config'].str.contains('rmoutlierTrue').value_counts()\n",
    "\n",
    "    stableTop10Configs = pd.DataFrame(stableTop10.groupby('config').size(), columns=['nobs']).reset_index().sort_values('nobs', ascending=False)\n",
    "    #print(stableTop10Configs)\n",
    "\n",
    "    configs = pd.DataFrame(scores['config'].unique(), columns=['config'])\n",
    "    #sns.barplot(stableTop10Configs)\n",
    "    stableTop10Configs = configs.merge(stableTop10Configs, how='left', on='config')\n",
    "    stableTop10Configs.loc[stableTop10Configs.nobs.isna(), 'nobs'] = 0\n",
    "    stableTop10Configs['nobs'] = stableTop10Configs['nobs'].astype(int) \n",
    "    stableTop10Configs = stableTop10Configs.sort_values('nobs', ascending=False)\n",
    "    stableTop10Configs\n",
    "\n",
    "\n",
    "    my_dpi = 200\n",
    "    fig = plt.figure(\n",
    "        figsize=(\n",
    "            #10, 8\n",
    "            1500/my_dpi, 1000/my_dpi\n",
    "        )\n",
    "    ) \n",
    "    sns.barplot(data=stableTop10Configs, x='nobs', y='config')\n",
    "    plt.yticks( fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(explorations_path, 'stableTop10Configs.png'), dpi=200)\n",
    "\n",
    "    plt.ylabel\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # split config description \n",
    "\n",
    "    splits = configs.config.str.split('_', expand=True)\n",
    "    print(splits.shape)\n",
    "    print(splits.iloc[:,:-1])\n",
    "\n",
    "    #['_'.join(line) for line in splits[0:3]]\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "    # config with highest score per model \n",
    "    idx = scores_comp.groupby('model')['mean'].transform(max) == scores_comp['mean']\n",
    "    scores_comp[idx]\n",
    "\n",
    "#-----------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
